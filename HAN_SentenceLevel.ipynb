{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN-SentenceLevel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltI7R57CtKjc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2CODR13t4k4"
      },
      "source": [
        "import re\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
        "import nltk\n",
        "import tensorflow.compat.v1 as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, GRU\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n",
        "from tensorflow.keras.layers import GRUCell\n",
        "from keras.engine.topology import Layer\n",
        "from keras.models import Model\n",
        "nltk.download('punkt')\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2EneGBvu2zR"
      },
      "source": [
        "maxlen = 80\n",
        "max_sentences = 15\n",
        "max_words = 20000\n",
        "embedding_dim = 100\n",
        "validation_split = 0.2\n",
        "hidden_size=150 \n",
        "attention_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SMg3kgJt992"
      },
      "source": [
        "def clean_text(text):\n",
        "  '''\n",
        "  Remove non-ascii characters, multiple spaces, and newlines\n",
        "  '''\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
        "  text = re.sub(r'\\n',' ', text)\n",
        "  text = re.sub(r\" +\",\" \",text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13M9R1GomwU"
      },
      "source": [
        "Clean the text and generate numpy arrays for instances and their labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p9rqAmSuENP"
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/TDL/Data/train.csv') # path to dataset\n",
        "X = []\n",
        "y = []\n",
        "reviews = []\n",
        "for index, row in data.iterrows():\n",
        "  cleaned = clean_text(row[1])\n",
        "  X.append(cleaned)\n",
        "  y.append(list(row[2::]))  \n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3JztpHFo13p"
      },
      "source": [
        "Generate a train and test set from the input dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbfHz7eyuFiq"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "data = np.zeros((X.shape[0], max_sentences, maxlen), dtype='int32')\n",
        "for i, review in enumerate(reviews):\n",
        "    for j, sentence in enumerate(review):\n",
        "        if j < max_sentences:\n",
        "            tokens = text_to_word_sequence(sentence)\n",
        "            k = 0\n",
        "            for _, word in enumerate(tokens):\n",
        "                if k < maxlen and tokenizer.word_index[word] < max_words:\n",
        "                    data[i, j, k] = tokenizer.word_index[word]\n",
        "                    k = k + 1\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = y[indices]\n",
        "nb_validation_samples = int(validation_split * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = y[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = y[-nb_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2CuIu0LVIPf"
      },
      "source": [
        "del X\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6C8SuJqB_K"
      },
      "source": [
        "Compute word embeddings for all words in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g29X7EJ_ueEz"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('drive/My Drive/TDL/glove.6B/glove.6B.100d.txt') # path to pre trained embeddings\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "del embeddings_index\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gpVHKAwqJtC"
      },
      "source": [
        "Define the word and sentence level attention layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iALQYec6Vjf1"
      },
      "source": [
        "def word_attention(inputs, att_size):\n",
        "    inputs = tf.concat(inputs, 2) # (n, embedding_size, hiddensize) -> (n, 80, 300)\n",
        "    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer -> 300\n",
        "    \n",
        "    # Trainable parameters\n",
        "    W_word = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1)) # (hiddensize, attsize) -> (300, 50)\n",
        "    b_word = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n",
        "    u_word = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n",
        "    v_word = tf.tanh(tf.tensordot(inputs, W_word, axes=1) + b_word)  # v = tanh(W.x + b) . (1,attsize) -> (n, 80, 50)\n",
        "    vu_word = tf.tensordot(v_word, u_word, axes=1, name='vu')  # v.u (n,80)\n",
        "    scores = tf.nn.softmax(vu_word)   # attention scores (n,80)\n",
        "\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(scores, -1), 1)\n",
        "    return output\n",
        "\n",
        "def sentence_attention(inputs, att_size):\n",
        "    inputs = tf.concat(inputs, 2) # (n, embedding_size, hiddensize) -> (n, 80, 300)\n",
        "    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer -> 300\n",
        "    \n",
        "    # Trainable parameters\n",
        "    W_sent = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1)) # (hiddensize, attsize) -> (300, 50)\n",
        "    b_sent = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n",
        "    u_sent = tf.Variable(tf.random_normal([att_size], stddev=0.1)) # (1, 50)\n",
        "    v_sent = tf.tanh(tf.tensordot(inputs, W_sent, axes=1) + b_sent)  # v = tanh(W.x + b) . (1,attsize) -> (n, 80, 50)\n",
        "    vu_sent = tf.tensordot(v_sent, u_sent, axes=1, name='vu')  # v.u (n,80)\n",
        "    scores = tf.nn.softmax(vu_sent)   # attention scores (n,80)\n",
        "\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(scores, -1), 1)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBGAnfjwqX-r"
      },
      "source": [
        "Define the Hierarchical Attention Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM2xpptRNNQH"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "sentence_input = tf.placeholder(tf.int32, [None,max_sentences, maxlen])\n",
        "input_len = tf.placeholder(tf.int32)\n",
        "output_x = tf.placeholder(tf.int32, [None,6])\n",
        "output_x = tf.cast(output_x,tf.float32) \n",
        "\n",
        "with tf.variable_scope(\"word\") as scope:\n",
        "  mat = []\n",
        "  for word in tf.unstack(sentence_input,axis=1):\n",
        "    embeddings_var = tf.Variable(embedding_matrix, trainable=True)\n",
        "    embeddings = tf.nn.embedding_lookup(embeddings_var, word, partition_strategy='div')\n",
        "    embeddings = tf.cast(embeddings,tf.float32) \n",
        "    rnn_outputs, _ = bidirectional_dynamic_rnn(GRUCell(hidden_size, dtype=tf.float32), GRUCell(hidden_size, dtype=tf.float32), inputs=embeddings, dtype=tf.float32)\n",
        "    weighted_inputs = word_attention(rnn_outputs,attention_size)\n",
        "    weighted_inputs = tf.reshape(weighted_inputs, [input_len, 1, weighted_inputs.shape[1]])\n",
        "    scope.reuse_variables()    \n",
        "    mat.append(weighted_inputs)\n",
        "\n",
        "after_word_attention = tf.stack(mat, axis=1)\n",
        "after_word_attention = tf.reshape(after_word_attention, [input_len, 15, 300])\n",
        "rnn_outputs_sent, _ = bidirectional_dynamic_rnn(GRUCell(round(hidden_size*1.5), dtype=tf.float32), GRUCell(round(hidden_size*1.5), dtype=tf.float32), inputs=after_word_attention, dtype=tf.float32)\n",
        "weighted_inputs_sent = sentence_attention(rnn_outputs_sent,attention_size)\n",
        "\n",
        "fc = tf.keras.layers.Dense(units=6, activation='sigmoid')(weighted_inputs_sent)\n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fc, labels=output_x)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n",
        "\n",
        "pred = tf.round(fc)\n",
        "n_correct = tf.equal(pred, output_x)\n",
        "accuracy = tf.reduce_mean(tf.cast(n_correct, tf.float32))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skxWazuK_25M"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  tf.initialize_all_variables().run()\n",
        "\n",
        "  # Training \n",
        "  epochs = 3\n",
        "  for epoch in range(epochs):\n",
        "    acc_sum = 0\n",
        "    n_batches = 0 \n",
        "    print(\"\\nEpoch: \",epoch+1)\n",
        "    for i in range(0,20000, 2000):\n",
        "      try:\n",
        "        x = x_train[i:i+200]\n",
        "        y = y_train[i:i+200]\n",
        "      except:\n",
        "        x = x_train[i::]\n",
        "        y = y_train[i::]\n",
        "      opt, acc = sess.run([optimizer,accuracy], feed_dict={sentence_input:x, output_x:y, input_len:len(y)})\n",
        "      acc_sum += acc\n",
        "      n_batches += 1\n",
        "      print(acc)\n",
        "    print('==> Train Accuracy: ', acc_sum/n_batches)\n",
        "  # Validation\n",
        "  val_accs=[]\n",
        "  for i in range(10):\n",
        "    val_accs.append(sess.run(accuracy, feed_dict={sentence_input:x_val[i*1000:(i+1)*1000], output_x:y_val[i*1000:(i+1)*1000], input_len:len(y_val[i*1000:(i+1)*1000])}))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tONiW5PqIYp1"
      },
      "source": [
        "np.mean(val_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3-abTeFM8NJ"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  tf.initialize_all_variables().run()\n",
        "  for i in range(10):\n",
        "    s = sess.run([embeddings, rnn_outputs, weighted_inputs, after_word_attention, rnn_outputs_sent, weighted_inputs_sent, fc, loss, accuracy, optimizer ], feed_dict={sentence_input:x_train[0:2], output_x:y_train[0:2]})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}