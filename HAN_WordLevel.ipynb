{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN-WordLevel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltI7R57CtKjc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2CODR13t4k4"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
        "import nltk\n",
        "import tensorflow.compat.v1 as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n",
        "from tensorflow.keras.layers import TimeDistributed as TD\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn\n",
        "from tensorflow.keras.layers import GRUCell\n",
        "nltk.download('punkt')\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2EneGBvu2zR"
      },
      "source": [
        "maxlen = 80\n",
        "max_sentences = 15 # max sentences per review\n",
        "max_words = 20000 # most common words (vocbaulary)\n",
        "embedding_dim = 100\n",
        "validation_split = 0.2\n",
        "hidden_size=150 \n",
        "attention_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SMg3kgJt992"
      },
      "source": [
        "def clean_text(text):\n",
        "  '''\n",
        "  Remove non-ascii characters, multiple spaces, and newlines\n",
        "  '''\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
        "  text = re.sub(r'\\n',' ', text)\n",
        "  text = re.sub(r\" +\",\" \",text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0NfTYu_v61v"
      },
      "source": [
        "X = np.load(\"drive/My Drive/TDL/Data/X.npy\")\n",
        "y = np.load(\"drive/My Drive/TDL/Data/y.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p9rqAmSuENP"
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/TDL/Data/train.csv')\n",
        "X = []\n",
        "y = []\n",
        "reviews = []\n",
        "for index, row in data.iterrows():\n",
        "  cleaned = clean_text(row[1]) # per review\n",
        "  X.append(cleaned)\n",
        "  y.append(list(row[2::]))  # (6,1)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbfHz7eyuFiq"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X) # X = ['hello' , 'hi', 'how', 'are']\n",
        "X = pad_sequences(X, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1rcPG9ssJ67"
      },
      "source": [
        "np.save(\"drive/My Drive/TDL/Data/X.npy\", X)\n",
        "np.save(\"drive/My Drive/TDL/Data/y.npy\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjP116_judyG"
      },
      "source": [
        "indices = np.arange(X.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "labels = y[indices]\n",
        "nb_validation_samples = int(validation_split * data.shape[0])\n",
        "\n",
        "x_train = X[:-nb_validation_samples]\n",
        "y_train = y[:-nb_validation_samples]\n",
        "x_val = X[-nb_validation_samples:]\n",
        "y_val = y[-nb_validation_samples:]\n",
        "\n",
        "del X\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g29X7EJ_ueEz"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('drive/My Drive/TDL/glove.6B/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "del embeddings_index\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iALQYec6Vjf1"
      },
      "source": [
        "def attention(inputs, att_size):\n",
        "    \"\"\"\n",
        "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
        "    \"\"\"\n",
        "    inputs = tf.concat(inputs, 2) # (n, embedding_size, hiddensize) -> (n, 80, 300)\n",
        "    hiddensize = inputs.shape[2].value  # D value - hidden size of the RNN layer -> 300\n",
        "    \n",
        "    # Trainable parameters\n",
        "    W = tf.Variable(tf.random_normal([hiddensize, att_size], stddev=0.1), trainable=True) # (hiddensize, attsize) -> (300, 50)\n",
        "    b = tf.Variable(tf.random_normal([att_size], stddev=0.1), trainable=True) # (1, 50)\n",
        "    u = tf.Variable(tf.random_normal([att_size], stddev=0.1), trainable=True) # (1, 50)\n",
        "    v = tf.tanh(tf.tensordot(inputs, W, axes=1) + b)  # v = tanh(W.x + b) . (1,attsize) -> (n, 80, 50)\n",
        "    vu = tf.tensordot(v, u, axes=1, name='vu')  # v.u (n,80)\n",
        "    scores = tf.nn.softmax(vu, name='alphas')   # attention scores (n,80)\n",
        "\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(scores, -1), 1)\n",
        "    return output, W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zQROBBLEd0"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# NEW NETWORK! EDIT THIS\n",
        "input_x = tf.placeholder(tf.int32, [None, maxlen])\n",
        "output_x = tf.placeholder(tf.float32, [None,6])\n",
        "embeddings_var = tf.Variable(embedding_matrix, trainable=True)\n",
        "embeddings = tf.nn.embedding_lookup(embeddings_var, input_x, partition_strategy='div')\n",
        "embed = tf.cast(embeddings,tf.float32)  # n*80*100\n",
        "\n",
        "rnn_outputs, _ = bidirectional_dynamic_rnn(GRUCell(hidden_size, dtype=tf.float32), GRUCell(hidden_size, dtype=tf.float32), inputs=embed, dtype=tf.float32)\n",
        "weighted_inputs, weights = attention(rnn_outputs,attention_size)\n",
        "\n",
        "fc = tf.keras.layers.Dense(units=6, activation='sigmoid')(weighted_inputs)\n",
        "\n",
        "output_x = tf.cast(output_x,tf.float32) \n",
        "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=fc, labels=output_x)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n",
        "\n",
        "pred = tf.round(fc)\n",
        "n_correct = tf.equal(pred, output_x)\n",
        "accuracy = tf.reduce_mean(tf.cast(n_correct, tf.float32))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvo341PD1vvD"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  tf.initialize_all_variables().run()\n",
        "  epochs = 10\n",
        "  for epoch in range(epochs):\n",
        "    acc_sum = 0\n",
        "    n_batches = 0 \n",
        "\n",
        "    print(\"epoch: \",epoch+1)\n",
        "    for i in range(0,x_train.shape[0], 256):\n",
        "      try:\n",
        "        x = x_train[i:i+256]\n",
        "        y = y_train[i:i+256]\n",
        "      except:\n",
        "        x = x_train[i::]\n",
        "        y = y_train[i::]\n",
        "      opt, acc = sess.run([optimizer,accuracy], feed_dict={input_x:x, output_x:y})\n",
        "      acc_sum += acc\n",
        "      n_batches += 1\n",
        "    print(acc_sum, n_batches)\n",
        "    print('Accuracy: ', acc_sum/n_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yar5nV9X3Q7u"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  tf.initialize_all_variables().run()\n",
        "  emb = sess.run(embeddings, feed_dict={input_x:x_train[0:2], output_x:y_train[0:2]})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}